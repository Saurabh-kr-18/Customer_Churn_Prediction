# Customer Churn Prediction

## ğŸ“Œ Overview
This project aims to predict customer churn using machine learning techniques on the **Telco Customer Churn Dataset**. The dataset includes customer demographics, account information, and service usage patterns. Our goal is to identify key drivers of churn and build predictive models that can flag customers at risk of leaving.

---

## ğŸ—‚ï¸ Project Structure
```
customer-churn-prediction/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv      # Raw dataset
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ Customer_Churn_Prediction_using_ML.ipynb  # Jupyter notebook for EDA & modeling
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing.py                          # Data preprocessing scripts
â”‚   â”œâ”€â”€ model.py                                  # Model training and evaluation
â”‚   â””â”€â”€ utils.py                                  # Helper functions
â”œâ”€â”€ README.md                                     # Project documentation
â”œâ”€â”€ requirements.txt                              # Python dependencies
```

---

## ğŸ“Š Dataset Information
- **Source**: Telco Customer Churn dataset
- **Rows**: 7,043  
- **Columns**: 21  
- **Target Variable**: `Churn` â€” indicates whether a customer has left the service.

The dataset contains customer demographics, account status, contract details, and service usage information.

---

## âš™ï¸ Installation

To set up the project locally:

```bash
# Clone the repository
git clone https://github.com/yourusername/customer-churn-prediction.git
cd customer-churn-prediction

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate        # On Windows use: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸš€ Usage

### To run the model training and evaluation notebook:

```bash
jupyter notebook
```

Then open:  
`notebooks/Customer_Churn_Prediction_using_ML.ipynb`  
It covers:
- Data exploration
- Preprocessing
- Model training & evaluation

---

## ğŸ”„ Data Preprocessing

The following steps were taken:
1. **Dropped** `customerID` as it has no predictive value.
2. **Encoded** categorical features using `LabelEncoder`.
3. **Balanced** the dataset using **SMOTE** (Synthetic Minority Over-sampling Technique) to address class imbalance.

---

## ğŸ¤– Machine Learning Models

### âœ”ï¸ Exploratory Data Analysis (EDA)
- Identified relationships between features and churn.
- Visualized trends in tenure, charges, and contract types.

### âœ”ï¸ Models Implemented
- **Decision Tree**: A simple model that splits data based on feature thresholds.
- **Random Forest**: Ensemble of decision trees to reduce overfitting and improve accuracy.
- **XGBoost**: Gradient boosting framework known for high performance and robustness.

### âœ”ï¸ Evaluation Metrics
- **Accuracy**
- **Confusion Matrix**
- **Precision, Recall, F1-score** (via Classification Report)
- **ROC Curve** and **AUC Score**

---

## ğŸ“ˆ Key Findings
- The **best-performing model** (XGBoost) achieved **~80% accuracy** on the test set.
- **Top influential features**:
  - `Tenure`
  - `MonthlyCharges`
  - `Contract` type

---

## ğŸ“Œ Future Improvements
- Hyperparameter tuning using GridSearchCV or Optuna.
- Incorporation of deep learning models (e.g., Neural Networks).
- Deployment via Flask or Streamlit for live predictions.

---
